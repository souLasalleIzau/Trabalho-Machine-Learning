{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Academy Machine Learning Foundations - Laboratório do Amazon Forecast\n",
    "\n",
    "## PARE! - Este arquivo é executado automaticamente na inicialização do laboratório. Não tente executar as células!\n",
    "\n",
    "Este notebook Jupyter faz parte do laboratório de alunos do Amazon Forecast. Ele foi executado quando o laboratório foi criado.\n",
    "\n",
    "Se você acabou de iniciar o laboratório, carregue `forecast-lab.ipynb` e trabalhe nesse notebook, em vez de utilizar este.\n",
    "\n",
    "Se você foi direcionado para esse arquivo, leia as células e as explicações. Evite executar as células.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo do notebook\n",
    "\n",
    "Este notebook carrega e pré-processa o conjunto de dados (dataset) de varejo on-line. Os dados são carregados no Amazon Simple Storage Service (Amazon S3), onde são usados para criar uma previsão por meio do Amazon Forecast. O notebook executa as seguintes etapas:\n",
    "\n",
    "- **Importação e funções** importa os pacotes usados e cria funções auxiliares.\n",
    "- **Importação de dados** faz download e carrega os dados em um DataFrame pandas.\n",
    "- **Pré-processamento de dados** filtra os dados que estão prontos para treinamento\n",
    "- **Geração de DataFrames de treinamento e de teste** faz um downsamples dos dados para uma frequência diária e divide o conjunto de dados (dataset) em DataFrames de treinamento e teste.\n",
    "- **Upload para o Amazon S3** faz upload dos DataFrames para o Amazon S3 como arquivos CSV (valores separados por vírgula).\n",
    "- **Criação do grupo de conjuntos de dados (dataset) do Amazon Forecast** cria o grupo de conjuntos de dados (dataset) do projeto.\n",
    "- **Criação dos conjuntos de dados (dataset)** cria os conjuntos de dados (datasets) no grupo de conjuntos de dados (dataset groups) e aguarda a conclusão da importação.\n",
    "- **Criação do previsor** treina o previsor usando o grupo de conjuntos de dados.\n",
    "- **Obtenção de métricas de precisão** exibe as métricas para o previsor.\n",
    "- **Criação do previsor** treina o previsor usando o grupo de conjuntos de dados (datasets).\n",
    "- **Limpeza opcional** pode executar uma limpeza se ela não for concluída no notebook `forecast-lab.ipynb`.\n",
    "\n",
    "Este notebook leva entre 60 e 90 minutos para ser executado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação e funções\n",
    "\n",
    "O código a seguir importa esses pacotes:\n",
    "\n",
    "- *boto3* representa o AWS SDK para Python (Boto3), que é a biblioteca Python da AWS\n",
    "- *pandas* fornece DataFrames para manipular dados de séries temporais\n",
    "- *matplotlib* fornece funções de plotagem\n",
    "- *sagemaker* representa a API necessária para trabalhar com o Amazon SageMaker\n",
    "- *time*,*sys*,*os*,*io* e *json* fornecem funções auxiliares \n",
    "\n",
    "Além disso, duas funções auxiliares são criadas:\n",
    "\n",
    "- `upload_s3_csv` faz upload de DataFrames pandas para o Amazon S3 como arquivos CSV. O cabeçalho é removido, mas *não* o índice.\n",
    "- `StatusIndicator` fornece uma função de status para chamadas de API de longa execução para o Amazon Forecast.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "bucket_name='c33334a421003l780155t1w26178775763-forecastbucket-y617mqtzdoyg'\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time, sys, os, io, json\n",
    "import sagemaker\n",
    "\n",
    "%store bucket_name\n",
    "\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(filename, header=False, index=True)\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=True )\n",
    "    s3_resource.Bucket(bucket_name).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "class StatusIndicator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.previous_status = None\n",
    "        self.need_newline = False\n",
    "        \n",
    "    def update( self, status ):\n",
    "        if self.previous_status != status:\n",
    "            if self.need_newline:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "            sys.stdout.write( status + \" \")\n",
    "            self.need_newline = True\n",
    "            self.previous_status = status\n",
    "        else:\n",
    "            sys.stdout.write(\".\")\n",
    "            self.need_newline = True\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def end(self):\n",
    "        if self.need_newline:\n",
    "            sys.stdout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação de dados\n",
    "\n",
    "A célula a seguir faz download do conjunto de dados, que é um arquivo do Microsoft Excel. Esse arquivo é carregado no pandas como um DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "session = boto3.Session()\n",
    "forecast = session.client(service_name='forecast') \n",
    "forecast_query = session.client(service_name='forecastquery')\n",
    "\n",
    "!aws s3 cp s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/forecast/ . --recursive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento de dados\n",
    "\n",
    "A célula a seguir conclui as seguintes etapas de pré-processamento:\n",
    "\n",
    "- Remove instâncias com valores ausentes\n",
    "- Define o índice para o recurso InvoiceDate\n",
    "- Remove instâncias que não são do Reino Unido\n",
    "- Remove instâncias que não usam o código de estoque de destino (21232)\n",
    "- Mantém instâncias em que o preço é maior que 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = pd.read_excel('online_retail_II.xlsx')\n",
    "retail = retail.dropna()\n",
    "retail['InvoiceDate'] = pd.to_datetime(retail.InvoiceDate)\n",
    "retail = retail.set_index('InvoiceDate')\n",
    "\n",
    "country_filter = ['United Kingdom']\n",
    "retail = retail[retail['Country'].isin(country_filter)]\n",
    "\n",
    "#stockcodes = ['ADJUST', 'ADJUST2', 'POST', 'M']\n",
    "#stockcodes = [21232,22423]\n",
    "stockcodes = [21232]\n",
    "retail = retail[retail.StockCode.isin(stockcodes)]\n",
    "\n",
    "retail = retail[retail['Price']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração de DataFrames de treinamento e teste\n",
    "\n",
    "A célula a seguir:\n",
    "\n",
    "- Divide os dados em séries temporais e pandas DataFrames de séries temporais relacionadas.\n",
    "- Faz o downsamples dos dados de vários registros de vendas por dia em um único valor diário. A coluna **Quantity** (Quantidade) é somada e a média é usada para a coluna **Price** (Preço).\n",
    "- Divide os DataFrames em um conjunto de dados (datasets) de treinamento de janeiro de 2010 a outubro de 2010 e um conjunto de dados (dataset) de teste de novembro de 2010 a dezembro de 2010.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retail_timeseries = retail[['StockCode','Quantity']]\n",
    "\n",
    "retail_timeseries = retail_timeseries.groupby('StockCode').resample('D').sum().reset_index().set_index(['InvoiceDate'])\n",
    "\n",
    "df_related_time_series = retail[['StockCode','Price']]\n",
    "df_related_time_series2 = df_related_time_series.groupby('StockCode').resample('D').mean().reset_index().set_index(['InvoiceDate'])\n",
    "df_related_time_series3 = df_related_time_series2.groupby('StockCode').pad()\n",
    "\n",
    "#df_related_time_series4 = df_related_time_series3.reset_index().set_index('InvoiceDate')\n",
    "\n",
    "# Select January to November for one DataFrame.\n",
    "jan_to_oct = retail_timeseries['2009-12':'2010-10']\n",
    "nov_to_dec = retail_timeseries['2010-11':'2010-12']\n",
    "jan_to_oct_related = df_related_time_series3['2009-12':'2010-10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload para o Amazon S3\n",
    "\n",
    "A célula a seguir faz upload dos DataFrames para o Amazon S3 usando a função auxiliar criada anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prefix='lab_4'\n",
    "train='retail_ts_train.csv'\n",
    "train_related='related_ts_train.csv'\n",
    "test='retail_ts_test.csv'\n",
    "\n",
    "key=prefix + '/forecast/' + train\n",
    "# key='lab_4_forecast_t/forecast/retail_time_series_train.csv'\n",
    "related_key = prefix + '/forecast/' + train_related\n",
    "# related_key='lab_4_forecast_t/forecast/related.csv'\n",
    "\n",
    "upload_s3_csv(train, 'forecast', jan_to_oct)\n",
    "upload_s3_csv(train_related, 'forecast', jan_to_oct_related)\n",
    "upload_s3_csv(test, 'forecast', nov_to_dec)\n",
    "\n",
    "dataset_frequency = \"D\" \n",
    "timestamp_format = \"yyyy-MM-dd\"\n",
    "\n",
    "# project = prefix\n",
    "dataset_name = prefix+'_ds'\n",
    "related_dataset_name = prefix+'_rds'\n",
    "dataset_group_name = prefix +'_dsg'\n",
    "\n",
    "s3_data_path = \"s3://\"+bucket_name+\"/\"+key\n",
    "s3_related_data_path = \"s3://\"+bucket_name+\"/\"+related_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%store prefix\n",
    "%store train\n",
    "%store test\n",
    "%store key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do grupo de conjuntos de dados (datsets group) do Amazon Forecast\n",
    "\n",
    "A célula a seguir cria o grupo de conjuntos de dados (datasets group) para a previsão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_group_response = forecast.create_dataset_group(DatasetGroupName=dataset_group_name,\n",
    "                                                              Domain=\"RETAIL\"\n",
    "                                                             )\n",
    "dataset_group_arn = create_dataset_group_response['DatasetGroupArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de conjuntos de dados (datasets)\n",
    "\n",
    "A célula a seguir cria as séries temporais e os conjuntos de dados (datasets) relacionados e os adiciona ao grupo de conjuntos de dados (datasets group).\n",
    "\n",
    "A célula aguardará o loop e exibirá o status até que os conjuntos de dados (datasets) sejam criados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#role_arn = 'arn:aws:iam::610073867519:role/service-role/AmazonForecast-ExecutionRole-1587767069972'\n",
    "iam = boto3.resource('iam')\n",
    "role_arn = iam.Role('forecast-role').arn\n",
    "\n",
    "# This is the schema of the time series dataset.\n",
    "schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"demand\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "\n",
    "time_series_response=forecast.create_dataset(\n",
    "                    Domain=\"RETAIL\",\n",
    "                    DatasetType='TARGET_TIME_SERIES',\n",
    "                    DatasetName=dataset_name,\n",
    "                    DataFrequency=dataset_frequency, \n",
    "                    Schema = schema\n",
    ")\n",
    "\n",
    "dataset_arn = time_series_response['DatasetArn']\n",
    "# forecast.describe_dataset(DatasetArn=dataset_arn)\n",
    "\n",
    "# Create the import job for the time series dataset.\n",
    "dataset_import_job_name = 'EP_DSIMPORT_JOB_TARGET'\n",
    "data_source = {\"S3Config\" : {\"Path\":s3_data_path,\"RoleArn\": role_arn} }\n",
    "ds_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName=dataset_import_job_name,\n",
    "                                                          DatasetArn=dataset_arn,\n",
    "                                                          DataSource= data_source,\n",
    "                                                          TimestampFormat=timestamp_format\n",
    "                                                         )\n",
    "\n",
    "ds_import_job_arn=ds_import_job_response['DatasetImportJobArn']\n",
    "\n",
    "# This is the schema of the related data, which contains the price.\n",
    "related_schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"price\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "\n",
    "related_time_series_response=forecast.create_dataset(\n",
    "                    Domain=\"RETAIL\",\n",
    "                    DatasetType='RELATED_TIME_SERIES',\n",
    "                    DatasetName=related_dataset_name,\n",
    "                    DataFrequency=dataset_frequency, \n",
    "                    Schema = related_schema\n",
    ")\n",
    "related_dataset_arn = related_time_series_response['DatasetArn']\n",
    "\n",
    "# forecast.describe_dataset(DatasetArn=related_dataset_arn)\n",
    "\n",
    "\n",
    "related_dataset_import_job_name = 'EP_DSIMPORT_JOB_TARGET_RELATED'\n",
    "\n",
    "related_data_source = {\"S3Config\" : {\"Path\":s3_related_data_path,\"RoleArn\": role_arn} }\n",
    "\n",
    "ds_related_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName=related_dataset_import_job_name,\n",
    "                                                          DatasetArn=related_dataset_arn,\n",
    "                                                          DataSource= related_data_source,\n",
    "                                                          TimestampFormat=timestamp_format\n",
    "                                                         )\n",
    "\n",
    "ds_related_import_job_arn=ds_related_import_job_response['DatasetImportJobArn']\n",
    "\n",
    "# Add the time series and related dataset to the dataset group.\n",
    "forecast.update_dataset_group(DatasetGroupArn=dataset_group_arn, DatasetArns=[dataset_arn, related_dataset_arn])\n",
    "#forecast.update_dataset_group(DatasetGroupArn=dataset_group_arn, DatasetArns=[dataset_arn])\n",
    "\n",
    "# Wait for the related dataset to finish.\n",
    "status_indicator = StatusIndicator()\n",
    "\n",
    "while True:\n",
    "    status = forecast.describe_dataset_import_job(DatasetImportJobArn=ds_related_import_job_arn)['Status']\n",
    "    status_indicator.update(status)\n",
    "    if status in ('ACTIVE', 'CREATE_FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "status_indicator.end()\n",
    "\n",
    "# Wait for the time series dataset to finish. This process typically takes longer than the related set.\n",
    "status_indicator = StatusIndicator()\n",
    "\n",
    "while True:\n",
    "    status = forecast.describe_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)['Status']\n",
    "    status_indicator.update(status)\n",
    "    if status in ('ACTIVE', 'CREATE_FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "status_indicator.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A célula a seguir armazena os Nomes de recurso da Amazon - Amazon Resource Names (ARNs) para os objetos de previsão (forecast objects) criados anteriormente. Eles podem ser carregados de outros notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store ds_import_job_arn\n",
    "%store dataset_arn\n",
    "%store dataset_group_arn\n",
    "%store related_dataset_arn\n",
    "%store ds_related_import_job_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do preditor\n",
    "\n",
    "A célula a seguir cria o preditor usando os seguintes parâmetros:\n",
    "\n",
    "- O horizonte de previsão é definido como *30 dias*.\n",
    "- *DeepAR+* é o algoritmo selecionado. Para obter mais informações, consulte [DeepAR+Algorithm] (https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-deeparplus.html) na documentação da AWS.\n",
    "- Os hiperparâmetros são especificados para o algoritmo. Esses hiperparâmetros foram gerados executando a previsão com PerformHPO definido como *true*. Isso criou um trabalho de ajuste (tuning job) de hiperparâmetros no modelo, que produziu os valores a seguir.\n",
    "- Uma única janela de retrocesso é usada por *30 dias*.\n",
    "- A configuração de dados de entrada é definida como o grupo de conjuntos de dados (dataset) criado anteriormente.\n",
    "- Feriados para o Reino Unido são adicionados como componentes (features) complementares.\n",
    "- Um pipeline de caracterização de componentes é criado para os componentes (features) relativos a preços. Para obter mais informações, consulte o tópico [Tratamento de valores ausentes] (https://docs.aws.amazon.com/forecast/latest/dg/howitworks-missing-values.html) na documentação da AWS.\n",
    "\n",
    "A célula aguardará o loop e exibirá o status até que os conjuntos de dados (datasets) sejam criados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictor_name= prefix+'_deeparp_algo'\n",
    "forecast_horizon = 30\n",
    "algorithm_arn = 'arn:aws:forecast:::algorithm/Deep_AR_Plus'\n",
    "\n",
    "training_parameters =  {'context_length': '172', \n",
    "                        'epochs': '500', \n",
    "                        'learning_rate': '0.00023391131837525837', \n",
    "                        'learning_rate_decay': '0.5', \n",
    "                        'likelihood': 'student-t', \n",
    "                        'max_learning_rate_decays': '0', \n",
    "                        'num_averaged_models': '1', \n",
    "                        'num_cells': '40', \n",
    "                        'num_layers': '2', \n",
    "                        'prediction_length': '30'}\n",
    "\n",
    "evaluation_parameters= {\"NumberOfBacktestWindows\": 1, \"BackTestWindowOffset\": 30}\n",
    "\n",
    "input_data_config = {\"DatasetGroupArn\": dataset_group_arn, \"SupplementaryFeatures\": [ {\"Name\": \"holiday\",\"Value\": \"UK\"} ]}\n",
    "                  \n",
    "featurization_config= {\"ForecastFrequency\": dataset_frequency,\n",
    "                      \"Featurizations\": \n",
    "                      [\n",
    "                          {\n",
    "                            \"AttributeName\": \"price\",\n",
    "                            \"FeaturizationPipeline\": [\n",
    "                                {\n",
    "                                    \"FeaturizationMethodName\": \"filling\",\n",
    "                                    \"FeaturizationMethodParameters\": {\n",
    "                                        \"middlefill\": \"median\",\n",
    "                                        \"backfill\": \"min\",\n",
    "                                        \"futurefill\": \"max\"               \n",
    "                                        }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                      ]}\n",
    "\n",
    "\n",
    "create_predictor_response=forecast.create_predictor(PredictorName = predictor_name, \n",
    "                                                  AlgorithmArn = algorithm_arn,\n",
    "                                                  ForecastHorizon = forecast_horizon,\n",
    "                                                  PerformAutoML = False,\n",
    "                                                  PerformHPO = False,\n",
    "                                                  EvaluationParameters= evaluation_parameters, \n",
    "                                                  InputDataConfig = input_data_config,\n",
    "                                                  FeaturizationConfig = featurization_config,\n",
    "                                                  TrainingParameters = training_parameters\n",
    "                                                 )\n",
    "\n",
    "predictor_arn = create_predictor_response['PredictorArn']\n",
    "status_indicator = StatusIndicator()\n",
    "\n",
    "while True:\n",
    "    status = forecast.describe_predictor(PredictorArn=predictor_arn)['Status']\n",
    "    status_indicator.update(status)\n",
    "    if status in ('ACTIVE', 'CREATE_FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "status_indicator.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = forecast.describe_predictor(PredictorArn=predictor_arn)\n",
    "print(f['TrainingParameters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtenção de métricas de precisão\n",
    "\n",
    "A próxima célula imprime as métricas de precisão para o preditor que acabou de ser criado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.get_accuracy_metrics(PredictorArn=predictor_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação da previsão (forecast)\n",
    "\n",
    "A célula a seguir cria uma previsão do preditor que foi criado anteriormente. \n",
    "\n",
    "Os valores de ARN do preditor e da predição são armazenados para que possam ser recuperados do notebook do laboratório.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_Name= prefix+'_deeparp_algo_forecast'\n",
    "create_forecast_response=forecast.create_forecast(ForecastName=forecast_Name,\n",
    "                                                  PredictorArn=predictor_arn)\n",
    "forecast_arn = create_forecast_response['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store forecast_arn\n",
    "%store predictor_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_indicator = StatusIndicator()\n",
    "while True:\n",
    "    status = forecast.describe_forecast(ForecastArn=forecast_arn)['Status']\n",
    "    status_indicator.update(status)\n",
    "    if status in ('ACTIVE', 'CREATE_FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "status_indicator.end()\n",
    "\n",
    "print(forecast_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A próxima célula cria uma previsão rápida como um teste, o que pode ser útil para a solução de problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "forecast_response = forecast_query.query_forecast(\n",
    "    ForecastArn=forecast_arn,\n",
    "    Filters={\"item_id\":\"21232\"}\n",
    ")\n",
    "print(forecast_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza opcional\n",
    "\n",
    "A limpeza é realizada no notebook `forecast-lab.ipynb`. Se você precisar executar a limpeza aqui, altere a célula a seguir para código selecionando a célula e pressionando S. Em seguida, execute a célula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forecast.delete_forecast(ForecastArn=forecast_arn)\n",
    "time.sleep(60)\n",
    "\n",
    "forecast.delete_predictor(PredictorArn=predictor_arn)\n",
    "time.sleep(60)\n",
    "\n",
    "forecast.delete_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)\n",
    "time.sleep(60)\n",
    "\n",
    "forecast.delete_dataset(DatasetArn=dataset_arn)\n",
    "time.sleep(60)\n",
    "\n",
    "forecast.delete_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 32-bit",
   "language": "python",
   "name": "python37432bitf4c4140064114c7793d4abb1ad5a732a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
